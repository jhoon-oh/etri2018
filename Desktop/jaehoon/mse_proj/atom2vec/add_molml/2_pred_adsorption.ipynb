{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('molml_data.csv')\n",
    "data = data.drop('0', axis=1)\n",
    "\n",
    "# Set threshold\n",
    "below_thr, above_thr = np.quantile(data['442'], [0.05, 0.95])\n",
    "tmp = []\n",
    "for i in range(1520):\n",
    "    energy = data.iloc[i]['442']\n",
    "    if energy < below_thr or energy > above_thr:\n",
    "        tmp.append(i)\n",
    "data = data.drop(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.drop('442', axis=1)\n",
    "selected_col = []\n",
    "for i in range(21):\n",
    "    for j in range(i, 21):\n",
    "        selected_col.append(str(21*i+(j+1)))\n",
    "input_data = input_data[selected_col]\n",
    "output_data = data['442']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for idx in list(input_data.index):\n",
    "    input_tmp = torch.tensor(list(input_data.loc[idx]))\n",
    "    output_tmp = torch.tensor(float(output_data.loc[idx]))\n",
    "    dataset.append([input_tmp, output_tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train/valid/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# gpu_option\n",
    "gpu_use = 1\n",
    "which_gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset):\n",
    "    torch.manual_seed(0)\n",
    "    train, valid, test = random_split(dataset, [968, 200, 200])\n",
    "    \n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = make_loader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pred_net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(231, 128)\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        self.hidden3 = nn.Linear(64, 32)\n",
    "        self.hidden4 = nn.Linear(32, 16)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = torch.tanh(self.hidden4(x))\n",
    "        out = self.out(x)\n",
    "        out = out.view(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50000+1\n",
    "learning_rate = 1e-5\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,train_loader,valid_loader,criterion,learning_rate,num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        acc = []\n",
    "        train_error = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            atom2vec = data[0].type(torch.FloatTensor).cuda(which_gpu)\n",
    "            real_energy = data[1].type(torch.FloatTensor).cuda(which_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_energy = model(atom2vec)\n",
    "            \n",
    "            loss = criterion(pred_energy, real_energy)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_error += loss.item()\n",
    "       \n",
    "        if epoch % 1000 == 0:\n",
    "            avg_train_error = train_error/len(train_loader)\n",
    "            print (\"epoch : [%d/%d], The average loss of train error %.4f\" % (epoch+1, num_epochs, avg_train_error))\n",
    "            eval_loss, output_all, label_all = eval(model, valid_loader, criterion)\n",
    "            scheduler.step(eval_loss)\n",
    "        \n",
    "def eval(model,valid_loader,criterion):\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    output_all = []\n",
    "    label_all = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        atom2vec = data[0].type(torch.FloatTensor).cuda(which_gpu)\n",
    "        real_energy = data[1].type(torch.FloatTensor).cuda(which_gpu)\n",
    "\n",
    "        pred_energy = model(atom2vec)\n",
    "        loss = criterion(pred_energy, real_energy)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        output_all.append(pred_energy.data.cpu().numpy())\n",
    "        label_all.append(real_energy.data.cpu().numpy())\n",
    "\n",
    "    avg_loss = eval_loss/len(valid_loader)\n",
    "    print ('The average loss of valid error: {:.4f} \\n'. format(avg_loss))\n",
    "\n",
    "    return avg_loss, output_all, label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : [1/50001], The average loss of train error 1.6185\n",
      "The average loss of valid error: 1.5759 \n",
      "\n",
      "epoch : [1001/50001], The average loss of train error 0.3205\n",
      "The average loss of valid error: 0.3764 \n",
      "\n",
      "epoch : [2001/50001], The average loss of train error 0.2760\n",
      "The average loss of valid error: 0.3884 \n",
      "\n",
      "epoch : [3001/50001], The average loss of train error 0.2231\n",
      "The average loss of valid error: 0.4091 \n",
      "\n",
      "epoch : [4001/50001], The average loss of train error 0.2052\n",
      "The average loss of valid error: 0.3692 \n",
      "\n",
      "epoch : [5001/50001], The average loss of train error 0.1875\n",
      "The average loss of valid error: 0.3859 \n",
      "\n",
      "epoch : [6001/50001], The average loss of train error 0.1924\n",
      "The average loss of valid error: 0.3342 \n",
      "\n",
      "epoch : [7001/50001], The average loss of train error 0.2055\n",
      "The average loss of valid error: 0.3494 \n",
      "\n",
      "epoch : [8001/50001], The average loss of train error 0.1893\n",
      "The average loss of valid error: 0.3493 \n",
      "\n",
      "epoch : [9001/50001], The average loss of train error 0.1807\n",
      "The average loss of valid error: 0.4275 \n",
      "\n",
      "epoch : [10001/50001], The average loss of train error 0.2014\n",
      "The average loss of valid error: 0.4119 \n",
      "\n",
      "epoch : [11001/50001], The average loss of train error 0.1687\n",
      "The average loss of valid error: 0.3757 \n",
      "\n",
      "epoch : [12001/50001], The average loss of train error 0.1731\n",
      "The average loss of valid error: 0.3733 \n",
      "\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch : [13001/50001], The average loss of train error 0.1662\n",
      "The average loss of valid error: 0.3838 \n",
      "\n",
      "epoch : [14001/50001], The average loss of train error 0.1585\n",
      "The average loss of valid error: 0.4186 \n",
      "\n",
      "epoch : [15001/50001], The average loss of train error 0.1554\n",
      "The average loss of valid error: 0.4344 \n",
      "\n",
      "epoch : [16001/50001], The average loss of train error 0.1492\n",
      "The average loss of valid error: 0.4319 \n",
      "\n",
      "epoch : [17001/50001], The average loss of train error 0.1422\n",
      "The average loss of valid error: 0.4398 \n",
      "\n",
      "epoch : [18001/50001], The average loss of train error 0.1394\n",
      "The average loss of valid error: 0.4124 \n",
      "\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-07.\n",
      "epoch : [19001/50001], The average loss of train error 0.1371\n",
      "The average loss of valid error: 0.4077 \n",
      "\n",
      "epoch : [20001/50001], The average loss of train error 0.1325\n",
      "The average loss of valid error: 0.3932 \n",
      "\n",
      "epoch : [21001/50001], The average loss of train error 0.1297\n",
      "The average loss of valid error: 0.4202 \n",
      "\n",
      "epoch : [22001/50001], The average loss of train error 0.1268\n",
      "The average loss of valid error: 0.4212 \n",
      "\n",
      "epoch : [23001/50001], The average loss of train error 0.1259\n",
      "The average loss of valid error: 0.4260 \n",
      "\n",
      "epoch : [24001/50001], The average loss of train error 0.1246\n",
      "The average loss of valid error: 0.4482 \n",
      "\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-08.\n",
      "epoch : [25001/50001], The average loss of train error 0.1252\n",
      "The average loss of valid error: 0.4498 \n",
      "\n",
      "epoch : [26001/50001], The average loss of train error 0.1258\n",
      "The average loss of valid error: 0.4468 \n",
      "\n",
      "epoch : [27001/50001], The average loss of train error 0.1233\n",
      "The average loss of valid error: 0.4487 \n",
      "\n",
      "epoch : [28001/50001], The average loss of train error 0.1233\n",
      "The average loss of valid error: 0.4497 \n",
      "\n",
      "epoch : [29001/50001], The average loss of train error 0.1237\n",
      "The average loss of valid error: 0.4515 \n",
      "\n",
      "epoch : [30001/50001], The average loss of train error 0.1236\n",
      "The average loss of valid error: 0.4529 \n",
      "\n",
      "epoch : [31001/50001], The average loss of train error 0.1218\n",
      "The average loss of valid error: 0.4555 \n",
      "\n",
      "epoch : [32001/50001], The average loss of train error 0.1235\n",
      "The average loss of valid error: 0.4568 \n",
      "\n",
      "epoch : [33001/50001], The average loss of train error 0.1218\n",
      "The average loss of valid error: 0.4570 \n",
      "\n",
      "epoch : [34001/50001], The average loss of train error 0.1219\n",
      "The average loss of valid error: 0.4580 \n",
      "\n",
      "epoch : [35001/50001], The average loss of train error 0.1209\n",
      "The average loss of valid error: 0.4577 \n",
      "\n",
      "epoch : [36001/50001], The average loss of train error 0.1215\n",
      "The average loss of valid error: 0.4572 \n",
      "\n",
      "epoch : [37001/50001], The average loss of train error 0.1201\n",
      "The average loss of valid error: 0.4570 \n",
      "\n",
      "epoch : [38001/50001], The average loss of train error 0.1215\n",
      "The average loss of valid error: 0.4566 \n",
      "\n",
      "epoch : [39001/50001], The average loss of train error 0.1208\n",
      "The average loss of valid error: 0.4564 \n",
      "\n",
      "epoch : [40001/50001], The average loss of train error 0.1240\n",
      "The average loss of valid error: 0.4562 \n",
      "\n",
      "epoch : [41001/50001], The average loss of train error 0.1209\n",
      "The average loss of valid error: 0.4563 \n",
      "\n",
      "epoch : [42001/50001], The average loss of train error 0.1212\n",
      "The average loss of valid error: 0.4565 \n",
      "\n",
      "epoch : [43001/50001], The average loss of train error 0.1238\n",
      "The average loss of valid error: 0.4565 \n",
      "\n",
      "epoch : [44001/50001], The average loss of train error 0.1218\n",
      "The average loss of valid error: 0.4565 \n",
      "\n",
      "epoch : [45001/50001], The average loss of train error 0.1209\n",
      "The average loss of valid error: 0.4560 \n",
      "\n",
      "epoch : [46001/50001], The average loss of train error 0.1193\n",
      "The average loss of valid error: 0.4555 \n",
      "\n",
      "epoch : [47001/50001], The average loss of train error 0.1201\n",
      "The average loss of valid error: 0.4552 \n",
      "\n",
      "epoch : [48001/50001], The average loss of train error 0.1188\n",
      "The average loss of valid error: 0.4554 \n",
      "\n",
      "epoch : [49001/50001], The average loss of train error 0.1199\n",
      "The average loss of valid error: 0.4560 \n",
      "\n",
      "epoch : [50001/50001], The average loss of train error 0.1189\n",
      "The average loss of valid error: 0.4570 \n",
      "\n",
      "--- 1112.3870823383331 seconds ---\n",
      "The average loss of valid error: 0.3452 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = pred_net().cuda(which_gpu)\n",
    "# run\n",
    "start_time = time.time()\n",
    "fit(model,train_loader,valid_loader,criterion,learning_rate,num_epochs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# evaluation\n",
    "avg_loss, pred_all, real_all = eval(model,test_loader,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857231 1.05948\n"
     ]
    }
   ],
   "source": [
    "idx = 70\n",
    "print (real_all[0][idx], pred_all[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
