{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input/Output split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('molml_data.csv')\n",
    "data = data.drop('0', axis=1)\n",
    "\n",
    "# Set threshold\n",
    "below_thr, above_thr = np.quantile(data['442'], [0.05, 0.95])\n",
    "tmp = []\n",
    "for i in range(1520):\n",
    "    energy = data.iloc[i]['442']\n",
    "    if energy < below_thr or energy > above_thr:\n",
    "        tmp.append(i)\n",
    "data = data.drop(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data.drop('442', axis=1)\n",
    "selected_col = []\n",
    "for i in range(21):\n",
    "    for j in range(i, 21):\n",
    "        selected_col.append(str(21*i+(j+1)))\n",
    "input_data = input_data[selected_col]\n",
    "output_data = data['442']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in list(input_data.index):\n",
    "    input_tmp = torch.tensor(list(input_data.loc[idx]))\n",
    "    output_tmp = torch.tensor(float(output_data.loc[idx]))\n",
    "    dataset.append([input_tmp, output_tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make train/valid/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# gpu_option\n",
    "gpu_use = 1\n",
    "which_gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loader(dataset):\n",
    "    torch.manual_seed(0)\n",
    "    train, valid, test = random_split(dataset, [int(len(dataset)*0.8), int(len(dataset)*0.1), int(len(dataset)*0.1)])\n",
    "    \n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = make_loader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pred_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pred_net, self).__init__()\n",
    "        self.hidden1 = nn.Linear(231, 128)\n",
    "        self.hidden2 = nn.Linear(128, 64)\n",
    "        self.hidden3 = nn.Linear(64, 32)\n",
    "        self.hidden4 = nn.Linear(32, 16)\n",
    "        self.out = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        x = torch.tanh(self.hidden3(x))\n",
    "        x = torch.tanh(self.hidden4(x))\n",
    "        out = self.out(x)\n",
    "        out = out.view(-1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50000+1\n",
    "learning_rate = 1e-5\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,train_loader,valid_loader,criterion,learning_rate,num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        acc = []\n",
    "        train_error = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader):\n",
    "            atom2vec = data[0].type(torch.FloatTensor).cuda(which_gpu)\n",
    "            real_energy = data[1].type(torch.FloatTensor).cuda(which_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred_energy = model(atom2vec)\n",
    "            \n",
    "            loss = criterion(pred_energy, real_energy)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_error += loss.item()\n",
    "       \n",
    "        if epoch % 1000 == 0:\n",
    "            avg_train_error = train_error/len(train_loader)\n",
    "            print (\"epoch : [%d/%d], The average loss of train error %.4f\" % (epoch+1, num_epochs, avg_train_error))\n",
    "            eval_loss, output_all, label_all = eval(model, valid_loader, criterion)\n",
    "            scheduler.step(eval_loss)\n",
    "        \n",
    "def eval(model,valid_loader,criterion):\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    output_all = []\n",
    "    label_all = []\n",
    "\n",
    "    model.eval()\n",
    "    for i, data in enumerate(valid_loader):\n",
    "        atom2vec = data[0].type(torch.FloatTensor).cuda(which_gpu)\n",
    "        real_energy = data[1].type(torch.FloatTensor).cuda(which_gpu)\n",
    "\n",
    "        pred_energy = model(atom2vec)\n",
    "        loss = criterion(pred_energy, real_energy)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        output_all.append(pred_energy.data.cpu().numpy())\n",
    "        label_all.append(real_energy.data.cpu().numpy())\n",
    "\n",
    "    avg_loss = eval_loss/len(valid_loader)\n",
    "    print ('The average loss of valid error: {:.4f} \\n'. format(avg_loss))\n",
    "\n",
    "    return avg_loss, output_all, label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : [1/50001], The average loss of train error 2.1276\n",
      "The average loss of valid error: 2.0075 \n",
      "\n",
      "epoch : [1001/50001], The average loss of train error 0.4993\n",
      "The average loss of valid error: 0.8131 \n",
      "\n",
      "epoch : [2001/50001], The average loss of train error 0.4092\n",
      "The average loss of valid error: 0.6648 \n",
      "\n",
      "epoch : [3001/50001], The average loss of train error 0.3267\n",
      "The average loss of valid error: 0.5536 \n",
      "\n",
      "epoch : [4001/50001], The average loss of train error 0.2971\n",
      "The average loss of valid error: 0.6020 \n",
      "\n",
      "epoch : [5001/50001], The average loss of train error 0.2830\n",
      "The average loss of valid error: 0.7908 \n",
      "\n",
      "epoch : [6001/50001], The average loss of train error 0.2447\n",
      "The average loss of valid error: 0.6033 \n",
      "\n",
      "epoch : [7001/50001], The average loss of train error 0.2875\n",
      "The average loss of valid error: 0.5132 \n",
      "\n",
      "epoch : [8001/50001], The average loss of train error 0.2597\n",
      "The average loss of valid error: 0.4916 \n",
      "\n",
      "epoch : [9001/50001], The average loss of train error 0.2639\n",
      "The average loss of valid error: 0.4748 \n",
      "\n",
      "epoch : [10001/50001], The average loss of train error 0.2814\n",
      "The average loss of valid error: 0.5257 \n",
      "\n",
      "epoch : [11001/50001], The average loss of train error 0.2789\n",
      "The average loss of valid error: 0.6196 \n",
      "\n",
      "epoch : [12001/50001], The average loss of train error 0.2481\n",
      "The average loss of valid error: 0.5938 \n",
      "\n",
      "epoch : [13001/50001], The average loss of train error 0.2679\n",
      "The average loss of valid error: 0.5576 \n",
      "\n",
      "epoch : [14001/50001], The average loss of train error 0.2654\n",
      "The average loss of valid error: 0.6287 \n",
      "\n",
      "epoch : [15001/50001], The average loss of train error 0.2665\n",
      "The average loss of valid error: 0.6937 \n",
      "\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-06.\n",
      "epoch : [16001/50001], The average loss of train error 0.2543\n",
      "The average loss of valid error: 0.6655 \n",
      "\n",
      "epoch : [17001/50001], The average loss of train error 0.2248\n",
      "The average loss of valid error: 0.5887 \n",
      "\n",
      "epoch : [18001/50001], The average loss of train error 0.2315\n",
      "The average loss of valid error: 0.7251 \n",
      "\n",
      "epoch : [19001/50001], The average loss of train error 0.2169\n",
      "The average loss of valid error: 0.5728 \n",
      "\n",
      "epoch : [20001/50001], The average loss of train error 0.2093\n",
      "The average loss of valid error: 0.5541 \n",
      "\n",
      "epoch : [21001/50001], The average loss of train error 0.1596\n",
      "The average loss of valid error: 0.5366 \n",
      "\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-07.\n",
      "epoch : [22001/50001], The average loss of train error 0.1547\n",
      "The average loss of valid error: 0.6022 \n",
      "\n",
      "epoch : [23001/50001], The average loss of train error 0.1416\n",
      "The average loss of valid error: 0.4864 \n",
      "\n",
      "epoch : [24001/50001], The average loss of train error 0.1449\n",
      "The average loss of valid error: 0.4827 \n",
      "\n",
      "epoch : [25001/50001], The average loss of train error 0.1364\n",
      "The average loss of valid error: 0.6148 \n",
      "\n",
      "epoch : [26001/50001], The average loss of train error 0.1346\n",
      "The average loss of valid error: 0.5838 \n",
      "\n",
      "epoch : [27001/50001], The average loss of train error 0.1422\n",
      "The average loss of valid error: 0.5941 \n",
      "\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-08.\n",
      "epoch : [28001/50001], The average loss of train error 0.1313\n",
      "The average loss of valid error: 0.5137 \n",
      "\n",
      "epoch : [29001/50001], The average loss of train error 0.1360\n",
      "The average loss of valid error: 0.5066 \n",
      "\n",
      "epoch : [30001/50001], The average loss of train error 0.1418\n",
      "The average loss of valid error: 0.4996 \n",
      "\n",
      "epoch : [31001/50001], The average loss of train error 0.1353\n",
      "The average loss of valid error: 0.4983 \n",
      "\n",
      "epoch : [32001/50001], The average loss of train error 0.1368\n",
      "The average loss of valid error: 0.6219 \n",
      "\n",
      "epoch : [33001/50001], The average loss of train error 0.1338\n",
      "The average loss of valid error: 0.5031 \n",
      "\n",
      "epoch : [34001/50001], The average loss of train error 0.1320\n",
      "The average loss of valid error: 0.4811 \n",
      "\n",
      "epoch : [35001/50001], The average loss of train error 0.1432\n",
      "The average loss of valid error: 0.5129 \n",
      "\n",
      "epoch : [36001/50001], The average loss of train error 0.1318\n",
      "The average loss of valid error: 0.5224 \n",
      "\n",
      "epoch : [37001/50001], The average loss of train error 0.1317\n",
      "The average loss of valid error: 0.4795 \n",
      "\n",
      "epoch : [38001/50001], The average loss of train error 0.1324\n",
      "The average loss of valid error: 0.5485 \n",
      "\n",
      "epoch : [39001/50001], The average loss of train error 0.1325\n",
      "The average loss of valid error: 0.4601 \n",
      "\n",
      "epoch : [40001/50001], The average loss of train error 0.1309\n",
      "The average loss of valid error: 0.5072 \n",
      "\n",
      "epoch : [41001/50001], The average loss of train error 0.1359\n",
      "The average loss of valid error: 0.4764 \n",
      "\n",
      "epoch : [42001/50001], The average loss of train error 0.1418\n",
      "The average loss of valid error: 0.6082 \n",
      "\n",
      "epoch : [43001/50001], The average loss of train error 0.1326\n",
      "The average loss of valid error: 0.5692 \n",
      "\n",
      "epoch : [44001/50001], The average loss of train error 0.1360\n",
      "The average loss of valid error: 0.5105 \n",
      "\n",
      "epoch : [45001/50001], The average loss of train error 0.1363\n",
      "The average loss of valid error: 0.4861 \n",
      "\n",
      "epoch : [46001/50001], The average loss of train error 0.1319\n",
      "The average loss of valid error: 0.4700 \n",
      "\n",
      "epoch : [47001/50001], The average loss of train error 0.1351\n",
      "The average loss of valid error: 0.4797 \n",
      "\n",
      "epoch : [48001/50001], The average loss of train error 0.1318\n",
      "The average loss of valid error: 0.6298 \n",
      "\n",
      "epoch : [49001/50001], The average loss of train error 0.1317\n",
      "The average loss of valid error: 0.5228 \n",
      "\n",
      "epoch : [50001/50001], The average loss of train error 0.1362\n",
      "The average loss of valid error: 0.4735 \n",
      "\n",
      "--- 956.4491195678711 seconds ---\n",
      "The average loss of valid error: 0.4197 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = pred_net().cuda(which_gpu)\n",
    "# run\n",
    "start_time = time.time()\n",
    "fit(model,train_loader,valid_loader,criterion,learning_rate,num_epochs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# evaluation\n",
    "avg_loss, pred_all, real_all = eval(model,test_loader,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.857231 1.05948\n"
     ]
    }
   ],
   "source": [
    "idx = 70\n",
    "print (real_all[0][idx], pred_all[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
